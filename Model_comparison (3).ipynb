{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RxDSysr1rPq"
      },
      "source": [
        "# Model comparison for the classification of moles on the HAM10000 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disclaimer: you need A LOT of RAM to run this script (>60Gbs)"
      ],
      "metadata": {
        "id": "uFOrF8FL8110"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0KEObSd1csH"
      },
      "source": [
        "import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JUd_z5h1cMj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, precision_recall_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from scipy import stats\n",
        "\n",
        "import itertools\n",
        "\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoiyYZpzjNvM",
        "outputId": "47f27dfb-7081-4ce5-d722-6f3d417c7a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0SZf_D5mfBO"
      },
      "source": [
        "## To facilitate variable naming, we will name each model:\n",
        "### NO segmentation NO hair removal == Model 1\n",
        "### NO segmentation YES hair removal == Model 2\n",
        "### YES segmentation YES hair removal == Model 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9hOm_peKpWZ"
      },
      "source": [
        "## Import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2NdjW6BbL7W"
      },
      "source": [
        "The datasplits are identical, what changes is the preprocessing of the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO1UtVSRJpYv"
      },
      "outputs": [],
      "source": [
        "X_1 = np.load('/content/drive/MyDrive/Project 36100 - Andrea, Monika, Yamuna/Assignment Stage 2/X_NO_dullrazor_NO_segmentation_128.npy')\n",
        "y_1 = np.load('/content/drive/MyDrive/Project 36100 - Andrea, Monika, Yamuna/Assignment Stage 2/y_NO_dullrazor_NO_segmentation_128.npy')\n",
        "\n",
        "X_2 = np.load('/content/drive/MyDrive/Project 36100 - Andrea, Monika, Yamuna/Assignment Stage 2/X_hair_removal_NO_segmentation_128.npy')\n",
        "y_2 = np.load('/content/drive/MyDrive/Project 36100 - Andrea, Monika, Yamuna/Assignment Stage 2/y_hair_removal_NO_segmentation_128.npy')\n",
        "\n",
        "X_3 = np.load('/content/drive/MyDrive/Project 36100 - Andrea, Monika, Yamuna/Assignment Stage 2/X_dullrazor_128_otsu.npy')\n",
        "y_3 = np.load('/content/drive/MyDrive/Project 36100 - Andrea, Monika, Yamuna/Assignment Stage 2/y_dullrazor_128_otsu.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpHRMtN5lwaA"
      },
      "source": [
        "One-hot encoding labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6T7fvyTlo05"
      },
      "outputs": [],
      "source": [
        "def prepare_labels(labels):\n",
        "    \"\"\"Convert string labels to one-hot encoding\"\"\"\n",
        "    #create and fit label encoder\n",
        "    label_encoder = LabelEncoder()\n",
        "    numeric_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "    #save label encoder classes so we can use them later for interpretation\n",
        "    label_mapping = dict(zip(label_encoder.classes_,\n",
        "                            range(len(label_encoder.classes_))))\n",
        "\n",
        "    #one-hot encoding the numeric-encoded classes\n",
        "    one_hot_labels = tf.keras.utils.to_categorical(numeric_labels)\n",
        "\n",
        "    #Print mapping for verification\n",
        "    print(\"Label mapping:\")\n",
        "    for label, idx in label_mapping.items():\n",
        "        print(f\"{label}: {idx}\")\n",
        "\n",
        "    return one_hot_labels, label_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nz74Bjdlz5y",
        "outputId": "bfd2d7fd-022d-440a-bc98-c72a1f656ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label mapping:\n",
            "akiec: 0\n",
            "bcc: 1\n",
            "bkl: 2\n",
            "df: 3\n",
            "mel: 4\n",
            "nv: 5\n",
            "vasc: 6\n",
            "Label mapping:\n",
            "akiec: 0\n",
            "bcc: 1\n",
            "bkl: 2\n",
            "df: 3\n",
            "mel: 4\n",
            "nv: 5\n",
            "vasc: 6\n",
            "Label mapping:\n",
            "akiec: 0\n",
            "bcc: 1\n",
            "bkl: 2\n",
            "df: 3\n",
            "mel: 4\n",
            "nv: 5\n",
            "vasc: 6\n"
          ]
        }
      ],
      "source": [
        "y_1_encoded, label_encoder = prepare_labels(y_1)\n",
        "y_2_encoded, _ = prepare_labels(y_2)\n",
        "y_3_encoded, _ = prepare_labels(y_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn6dOGdgkcyv"
      },
      "outputs": [],
      "source": [
        "#define splits\n",
        "##---Data splitting: we want a 75, 20, 5 train/test/validation split\n",
        "def datasplits(X, y_encoded):\n",
        "\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y_encoded,\n",
        "        test_size=0.20,\n",
        "        random_state=42, #for reproductibility\n",
        "        stratify=y_encoded\n",
        "    )\n",
        "\n",
        "      #Then split remaining data into train and validation (val is 5% of total)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val,\n",
        "        test_size=0.0625,  #0.05/0.80 to get 5% of total data\n",
        "        random_state=42,\n",
        "        stratify=y_train_val\n",
        "    )\n",
        "    return X_train, X_test, X_val, y_train, y_test, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIAHd8RVmaaP"
      },
      "outputs": [],
      "source": [
        "#all of the y_train, test and val are the same\n",
        "X_1_train, X_1_test, X_1_val, y_train, y_test, y_val = datasplits(X_1, y_1_encoded)\n",
        "X_2_train, X_2_test, X_2_val, _, _, _ = datasplits(X_2, y_2_encoded)\n",
        "X_3_train, X_3_test, X_3_val, _, _, _ = datasplits(X_3, y_3_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6LSn6xx1y9T"
      },
      "source": [
        "## Model evaluation function - We use McNemar's test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ziCaATFJkel"
      },
      "outputs": [],
      "source": [
        "def mcnemar_test(y_true, pred1, pred2):\n",
        "    \"\"\"\n",
        "    Perform McNemar's test to compare two models.\n",
        "    Returns test statistic, p-value, and contingency table.\n",
        "    \"\"\"\n",
        "    #Creation of the contingency table\n",
        "    correct1 = pred1 == y_true\n",
        "    correct2 = pred2 == y_true\n",
        "\n",
        "    #Contingency table values\n",
        "    b = np.sum(~correct1 & correct2)  #model1 wrong, model2 right\n",
        "    c = np.sum(correct1 & ~correct2)  #model1 right, model2 wrong\n",
        "\n",
        "    #Calculate test statistic and p-value\n",
        "    statistic = (abs(b - c) - 1)**2 / (b + c)\n",
        "    p_value = stats.chi2.sf(statistic, df=1)\n",
        "\n",
        "    contingency_table = {\n",
        "        'model1_right_model2_right': np.sum(correct1 & correct2),\n",
        "        'model1_right_model2_wrong': c,\n",
        "        'model1_wrong_model2_right': b,\n",
        "        'model1_wrong_model2_wrong': np.sum(~correct1 & ~correct2)\n",
        "    }\n",
        "\n",
        "    return statistic, p_value, contingency_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2ay4Kwi1apB"
      },
      "outputs": [],
      "source": [
        "def compare_models(models, X_test_dict, y_test, label_encoder, model_names=None):\n",
        "    \"\"\"\n",
        "    Compare multiple models with statistical testing.\n",
        "    \"\"\"\n",
        "    if model_names is None:\n",
        "        model_names = [f'Model_{i}' for i in range(len(models))]\n",
        "\n",
        "    # Perform pairwise statistical tests\n",
        "    pairwise_results = []\n",
        "    for i, model1 in enumerate(models):\n",
        "        for j in range(i + 1, len(models)):  #Start j from i+1 to avoid redundant comparisons\n",
        "            model2 = models[j]\n",
        "            #get correct datasplit\n",
        "            X_test_1 = X_test_dict[model_names[i]]\n",
        "            X_test_2 = X_test_dict[model_names[j]]\n",
        "\n",
        "            #Model predictions\n",
        "            model_pred_1 = np.argmax(model1.predict(X_test_1), axis=1)\n",
        "            model_pred_2 = np.argmax(model2.predict(X_test_2), axis=1)\n",
        "            return model_pred_1, model_pred_2\n",
        "            statistic, p_value, contingency = mcnemar_test(\n",
        "                y_test,\n",
        "                model_pred_1,\n",
        "                model_pred_2\n",
        "            )\n",
        "\n",
        "            result = {\n",
        "                'Model 1': model_names[i],\n",
        "                'Model 2': model_names[j],\n",
        "                'Test Statistic': statistic,\n",
        "                'P-value': p_value,\n",
        "                'Significant': p_value < 0.05,\n",
        "                **contingency\n",
        "            }\n",
        "            pairwise_results.append(result)\n",
        "\n",
        "    statistical_tests = pd.DataFrame(pairwise_results)\n",
        "    return statistical_tests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlhK3iqmo_gM"
      },
      "source": [
        "## Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK3f9vwRpB0C"
      },
      "outputs": [],
      "source": [
        "#model 1\n",
        "frozen_1 = load_model('/content/drive/MyDrive/Project 36100 - Andrea, Monika, Yamuna/Assignment Stage 2/Frozen_model/densenet201_ph1_128_no_dullrazor_no_segmentation.keras')\n",
        "fine_tuned_1 = load_model('/content/drive/MyDrive/Project 36100 - Andrea, Monika, Yamuna/Assignment Stage 2/Fine_tuned_model/densenet201_ph2_128_no_dullrazor_no_segmentation.keras')\n",
        "\n",
        "#model 2\n",
        "frozen_2 = load_model('/content/drive/MyDrive/Project 36100 - Andrea, Monika, Yamuna/Assignment Stage 2/Frozen_model/densenet201_ph1_128_no_segmentation.keras')\n",
        "fine_tuned_2 = load_model('/content/drive/MyDrive/Project 36100 - Andrea, Monika, Yamuna/Assignment Stage 2/Frozen_model/densenet201_ph1_128_no_segmentation.keras')\n",
        "\n",
        "#model 3\n",
        "frozen_3 = load_model('/content/drive/MyDrive/Project 36100 - Andrea, Monika, Yamuna/Assignment Stage 2/Fine_tuned_model/densenet201_ph2_128_dullrazor_segmented.keras')\n",
        "fine_tuned_3 = load_model('/content/drive/MyDrive/Project 36100 - Andrea, Monika, Yamuna/Assignment Stage 2/Frozen_model/densenet201_ph1_128_dullrazor_segmented.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufN_gSbf5h3Y"
      },
      "outputs": [],
      "source": [
        "#compare models\n",
        "models = [frozen_1, fine_tuned_1]\n",
        "model_names = [\"F_1\", \"FT_1\"]\n",
        "X_test_dict = {\"F_1\":X_1_test, \"FT_1\":X_1_test}\n",
        "\n",
        "model_pred_1, model_pred_2 = compare_models(models, X_test_dict, y_test, label_encoder, model_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u045p5Yo5lrv"
      },
      "outputs": [],
      "source": [
        "y_true = np.argmax(y_test, axis=1)\n",
        "mcnemar_test(y_true, model_pred_1, model_pred_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_tuQGwka-Ba"
      },
      "outputs": [],
      "source": [
        "#compare models\n",
        "models = [frozen_1, fine_tuned_1]\n",
        "model_names = [\"F_1\", \"FT_1\"]\n",
        "X_test_dict = {\"F_1\":X_1_test, \"FT_1\":X_1_test}\n",
        "\n",
        "comparison_df, detailed_metrics, statistical_tests = compare_models(models, X_test_dict, y_test, label_encoder, model_names)\n",
        "# Print the basic comparison\n",
        "print(\"\\nModel Comparison Summary:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Print statistical test results\n",
        "print(\"\\nPairwise Statistical Tests:\")\n",
        "print(statistical_tests.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}